{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93eb4491",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" # put -1 to not use any\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/lumargot/SurgicalSAM/segment-anything')\n",
    "sys.path.append('/home/lumargot/hysterectomy-coach/src/py')\n",
    "\n",
    "sys.path.append('/home/lumargot/SurgicalSAM/surgicalSAM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdfb1666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch-metric-learning in /NIRAL/tools/anaconda3/envs/flyby/lib/python3.9/site-packages (2.8.1)\n",
      "Requirement already satisfied: torch>=1.6.0 in /NIRAL/tools/anaconda3/envs/flyby/lib/python3.9/site-packages (from pytorch-metric-learning) (2.0.1)\n",
      "Requirement already satisfied: numpy in /NIRAL/tools/anaconda3/envs/flyby/lib/python3.9/site-packages (from pytorch-metric-learning) (1.24.2)\n",
      "Requirement already satisfied: scikit-learn in /NIRAL/tools/anaconda3/envs/flyby/lib/python3.9/site-packages (from pytorch-metric-learning) (1.2.0)\n",
      "Requirement already satisfied: tqdm in /home/lumargot/.local/lib/python3.9/site-packages (from pytorch-metric-learning) (4.64.1)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/lumargot/.local/lib/python3.9/site-packages (from torch>=1.6.0->pytorch-metric-learning) (8.5.0.96)\n",
      "Requirement already satisfied: sympy in /home/lumargot/.local/lib/python3.9/site-packages (from torch>=1.6.0->pytorch-metric-learning) (1.12)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/lumargot/.local/lib/python3.9/site-packages (from torch>=1.6.0->pytorch-metric-learning) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/lumargot/.local/lib/python3.9/site-packages (from torch>=1.6.0->pytorch-metric-learning) (10.9.0.58)\n",
      "Requirement already satisfied: filelock in /NIRAL/tools/anaconda3/envs/flyby/lib/python3.9/site-packages (from torch>=1.6.0->pytorch-metric-learning) (3.12.2)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/lumargot/.local/lib/python3.9/site-packages (from torch>=1.6.0->pytorch-metric-learning) (11.7.99)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /home/lumargot/.local/lib/python3.9/site-packages (from torch>=1.6.0->pytorch-metric-learning) (2.14.3)\n",
      "Requirement already satisfied: typing-extensions in /NIRAL/tools/anaconda3/envs/flyby/lib/python3.9/site-packages (from torch>=1.6.0->pytorch-metric-learning) (4.12.2)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /home/lumargot/.local/lib/python3.9/site-packages (from torch>=1.6.0->pytorch-metric-learning) (10.2.10.91)\n",
      "Requirement already satisfied: triton==2.0.0 in /home/lumargot/.local/lib/python3.9/site-packages (from torch>=1.6.0->pytorch-metric-learning) (2.0.0)\n",
      "Requirement already satisfied: networkx in /home/lumargot/.local/lib/python3.9/site-packages (from torch>=1.6.0->pytorch-metric-learning) (3.0)\n",
      "Requirement already satisfied: jinja2 in /home/lumargot/.local/lib/python3.9/site-packages (from torch>=1.6.0->pytorch-metric-learning) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /home/lumargot/.local/lib/python3.9/site-packages (from torch>=1.6.0->pytorch-metric-learning) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/lumargot/.local/lib/python3.9/site-packages (from torch>=1.6.0->pytorch-metric-learning) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /home/lumargot/.local/lib/python3.9/site-packages (from torch>=1.6.0->pytorch-metric-learning) (11.7.91)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /home/lumargot/.local/lib/python3.9/site-packages (from torch>=1.6.0->pytorch-metric-learning) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /home/lumargot/.local/lib/python3.9/site-packages (from torch>=1.6.0->pytorch-metric-learning) (11.7.101)\n",
      "Requirement already satisfied: wheel in /NIRAL/tools/anaconda3/envs/flyby/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.6.0->pytorch-metric-learning) (0.37.1)\n",
      "Requirement already satisfied: setuptools in /NIRAL/tools/anaconda3/envs/flyby/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.6.0->pytorch-metric-learning) (63.4.1)\n",
      "Requirement already satisfied: lit in /home/lumargot/.local/lib/python3.9/site-packages (from triton==2.0.0->torch>=1.6.0->pytorch-metric-learning) (16.0.6)\n",
      "Requirement already satisfied: cmake in /home/lumargot/.local/lib/python3.9/site-packages (from triton==2.0.0->torch>=1.6.0->pytorch-metric-learning) (3.27.4.1)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /NIRAL/tools/anaconda3/envs/flyby/lib/python3.9/site-packages (from scikit-learn->pytorch-metric-learning) (1.9.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/lumargot/.local/lib/python3.9/site-packages (from scikit-learn->pytorch-metric-learning) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /NIRAL/tools/anaconda3/envs/flyby/lib/python3.9/site-packages (from scikit-learn->pytorch-metric-learning) (3.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/lumargot/.local/lib/python3.9/site-packages (from jinja2->torch>=1.6.0->pytorch-metric-learning) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/lumargot/.local/lib/python3.9/site-packages (from sympy->torch>=1.6.0->pytorch-metric-learning) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install pytorch-metric-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32cfcb25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lumargot/.local/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from loss import DiceLoss\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from pytorch_metric_learning import losses as pml_losses\n",
    "from model_forward import model_forward_function\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "from segment_anything.utils.transforms import ResizeLongestSide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fef7c115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip uninstall -y segment_anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79d1986d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tools/anaconda3/envs/flyby/lib/python3.9/site-packages/albumentations/__init__.py:28: UserWarning: A new version of Albumentations is available: '2.0.7' (you have '2.0.3'). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import albumentations as A\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdc873ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import SimpleITK as sitk\n",
    "import numpy as np\n",
    "import random\n",
    "from model import Learnable_Prototypes, Prototype_Prompt_Encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea461283",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HystDatasetSAMPrompt(Dataset):\n",
    "    def __init__(self, df, mount_point=\"./\", transform=None, img_column=\"img_path\", seg_column='seg_path',\n",
    "                 class_column='class_column', predictor=None):\n",
    "        self.df = df\n",
    "        self.mount_point = mount_point\n",
    "        self.transform = transform\n",
    "        self.img_column = img_column\n",
    "        self.seg_column = seg_column\n",
    "        self.class_column = class_column\n",
    "        self.predictor = predictor\n",
    "\n",
    "        self.df_subject = self.df[self.img_column].drop_duplicates().reset_index()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df_subject)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        subject = self.df_subject.iloc[idx][self.img_column]\n",
    "        img_path = os.path.join(self.mount_point, subject)\n",
    "        df_patches = self.df.loc[self.df[self.img_column] == subject]\n",
    "\n",
    "        # Load image\n",
    "        img = sitk.GetArrayFromImage(sitk.ReadImage(img_path)).astype(np.float32).squeeze() / 255.0\n",
    "        shape = img.shape[:2]\n",
    "\n",
    "        masks, bboxes, labels = [], [], []\n",
    "\n",
    "        for _, row in df_patches.iterrows():\n",
    "            label = row[self.class_column]\n",
    "            seg_path = os.path.join(self.mount_point, row[self.seg_column])\n",
    "            seg = sitk.GetArrayFromImage(sitk.ReadImage(seg_path)).astype(np.float32).squeeze()\n",
    "\n",
    "            x, y, w, h = row['x'] * shape[1], row['y'] * shape[0], row['w'] * shape[1], row['h'] * shape[0]\n",
    "            bbox = [np.clip(x, 0, shape[1] - 5),\n",
    "                    np.clip(y, 0, shape[0] - 5),\n",
    "                    np.clip(x + w + 1, 5, shape[1]),\n",
    "                    np.clip(y + h + 1, 5, shape[0])]\n",
    "\n",
    "            masks.append(seg)\n",
    "            bboxes.append(bbox)\n",
    "            labels.append(label)\n",
    "\n",
    "        if len(masks) == 0:\n",
    "            return self.__getitem__(random.randint(0, len(self) - 1))\n",
    "\n",
    "        d_aug = self.transform(image=img, bboxes=bboxes, category_ids=labels, mask=np.stack(masks))\n",
    "        img = d_aug['image']\n",
    "        masks = d_aug['mask']\n",
    "        bboxes = d_aug['bboxes']\n",
    "\n",
    "        img_tensor = torch.tensor(img).permute(2, 0, 1)  # (1, 3, H, W)\n",
    "        mask_tensor = torch.tensor(masks).permute(2, 0, 1)\n",
    "\n",
    "        # obtain SAM feature of the augmented frame\n",
    "        self.predictor.set_image(img_tensor)\n",
    "        feat = self.predictor.features.squeeze() #\n",
    "\n",
    "\n",
    "        class_embeddings = []\n",
    "\n",
    "        for mask, label in zip(masks, labels):\n",
    "            mask_tensor = torch.tensor(mask, dtype=torch.float32)\n",
    "\n",
    "            mask_tensor = mask_tensor.unsqueeze(0).unsqueeze(0)  # [1, 1, H, W]\n",
    "            mask_resized = F.interpolate(mask_tensor, size=(64,64), mode='bilinear', align_corners=False)\n",
    "            mask_resized = mask_resized.squeeze()  # shape [H, W]\n",
    "\n",
    "            if torch.sum(mask_resized > 0.1) == 0:\n",
    "                continue\n",
    "\n",
    "            foreground_mask = mask_resized > 0.1  # Threshold to binarize\n",
    "            class_feat_vectors = feat[:, foreground_mask]  # shape [C, N_pixels]\n",
    "            cls_emb = class_feat_vectors.mean(dim=1)  # shape [C]\n",
    "\n",
    "\n",
    "            class_embeddings.append(cls_emb)\n",
    "\n",
    "        class_embeddings = torch.stack(class_embeddings).unsqueeze(0).transpose(1,2)\n",
    "        return feat.permute(1, 2, 0).unsqueeze(0), torch.tensor(labels).unsqueeze(0), masks, class_embeddings, img_tensor.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "206a0846",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HystDataModuleSAMPrompt(pl.LightningDataModule):\n",
    "    def __init__(self, df_train, df_val, df_test, mount_point=\"./\", batch_size=16, num_workers=4,\n",
    "                 img_column=\"img_path\", seg_column=\"seg_path\", class_column=\"class_column\",\n",
    "                 train_transform=None, valid_transform=None, test_transform=None,sam_checkpoint='model.ckpt',\n",
    "                 drop_last=False):\n",
    "        super().__init__()\n",
    "        self.df_train = df_train\n",
    "        self.df_val = df_val\n",
    "        self.df_test = df_test\n",
    "        self.mount_point = mount_point\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.img_column = img_column\n",
    "        self.seg_column = seg_column\n",
    "        self.class_column = class_column\n",
    "        self.train_transform = train_transform\n",
    "        self.valid_transform = valid_transform\n",
    "        self.test_transform = test_transform\n",
    "        self.drop_last = drop_last\n",
    "\n",
    "        \n",
    "        sam = sam_model_registry[f\"vit_h\"](checkpoint=sam_checkpoint)\n",
    "        sam.cuda()\n",
    "        self.predictor = SamPredictor(sam)\n",
    "\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.train_ds = HystDatasetSAMPrompt(self.df_train, mount_point=self.mount_point,\n",
    "                                             img_column=self.img_column, seg_column=self.seg_column,\n",
    "                                             class_column=self.class_column, transform=self.train_transform,predictor=self.predictor,\n",
    "                                             )\n",
    "\n",
    "        self.val_ds = HystDatasetSAMPrompt(self.df_val, mount_point=self.mount_point,\n",
    "                                           img_column=self.img_column, seg_column=self.seg_column,\n",
    "                                           class_column=self.class_column, transform=self.valid_transform,predictor=self.predictor,\n",
    "                                           )\n",
    "\n",
    "        self.test_ds = HystDatasetSAMPrompt(self.df_test, mount_point=self.mount_point,\n",
    "                                            img_column=self.img_column, seg_column=self.seg_column,\n",
    "                                            class_column=self.class_column, transform=self.test_transform,predictor=self.predictor,\n",
    "                                            )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_ds, batch_size=self.batch_size, shuffle=True,\n",
    "                          num_workers=self.num_workers, drop_last=self.drop_last)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_ds, batch_size=self.batch_size,\n",
    "                          num_workers=self.num_workers, drop_last=self.drop_last)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_ds, batch_size=self.batch_size,\n",
    "                          num_workers=self.num_workers, drop_last=self.drop_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5757419",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BBXImageTrainTransform():\n",
    "    def __init__(self):\n",
    "\n",
    "        self.transform = A.Compose(\n",
    "            [\n",
    "                A.LongestMaxSize(max_size_hw=(480, None)),\n",
    "                A.CenterCrop(height=480, width=836, pad_if_needed=True),\n",
    "                A.HorizontalFlip(),\n",
    "                A.GaussNoise(),\n",
    "                A.OneOf(\n",
    "                    [\n",
    "                        A.MotionBlur(p=.2),\n",
    "                        A.MedianBlur(blur_limit=3, p=0.1),\n",
    "                        A.Blur(blur_limit=3, p=0.1),\n",
    "                    ], p=0.2),\n",
    "                A.OneOf(\n",
    "                    [\n",
    "                        A.OpticalDistortion(p=0.3),\n",
    "                        A.GridDistortion(p=.1),\n",
    "                        ], p=0.2),\n",
    "                A.OneOf(\n",
    "                    [\n",
    "                        A.CLAHE(clip_limit=2),\n",
    "                        A.RandomBrightnessContrast(),\n",
    "                    ], p=0.3),\n",
    "                A.HueSaturationValue(p=0.3),\n",
    "                A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=1.0),\n",
    "\n",
    "            ], \n",
    "            bbox_params=A.BboxParams(format='pascal_voc', min_area=32, min_visibility=0.1, label_fields=['category_ids']),\n",
    "            additional_targets={'mask': 'masks'}\n",
    "        )\n",
    "\n",
    "    def __call__(self, image, bboxes, category_ids, mask):\n",
    "        return self.transform(image=image, bboxes=bboxes, category_ids=category_ids, mask=mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ffdf8179",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('/CMF/data/lumargot/hysterectomy/mnt/surgery_tracking/csv/dataset_test.csv')\n",
    "df_train = pd.read_csv('/CMF/data/lumargot/hysterectomy/mnt/surgery_tracking/csv/dataset_train_train.csv')\n",
    "df_val = pd.read_csv('/CMF/data/lumargot/hysterectomy/mnt/surgery_tracking/csv/dataset_train_test.csv')\n",
    "sam_checkpoint = \"/home/lumargot/SurgicalSAM/ckp/sam/sam_vit_h_4b8939.pth\"\n",
    "\n",
    "df_labels = pd.concat([df_train, df_val, df_test])\n",
    "\n",
    "img_column = 'img_path'\n",
    "seg_column = 'seg_path'\n",
    "class_column = 'simplified_class'\n",
    "label_column = 'simplified_label'\n",
    "mount_point = '/CMF/data/lumargot/hysterectomy/mnt/surgery_tracking/'\n",
    "\n",
    "\n",
    "ttdata = HystDataModuleSAMPrompt( df_test, df_test, df_test, batch_size=1, num_workers=1, \n",
    "                            img_column=img_column,seg_column=seg_column, class_column=class_column, \n",
    "                            mount_point=mount_point,train_transform=BBXImageTrainTransform(),\n",
    "                            valid_transform=BBXImageTrainTransform(), \n",
    "                            test_transform=BBXImageTrainTransform(), sam_checkpoint=sam_checkpoint)\n",
    "\n",
    "ttdata.setup()\n",
    "\n",
    "test_dl = ttdata.test_dataloader()\n",
    "test_ds = ttdata.test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "de174415",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "from einops import rearrange\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pdb\n",
    "\n",
    "class Learnable_Prototypes(nn.Module):\n",
    "    def __init__(self, num_classes=7 , feat_dim=256):\n",
    "        super(Learnable_Prototypes, self).__init__()\n",
    "        self.class_embeddings = nn.Embedding(num_classes, feat_dim)\n",
    "        \n",
    "    def forward(self):\n",
    "        return self.class_embeddings.weight\n",
    "\n",
    "class Prototype_Prompt_Encoder(nn.Module):\n",
    "    def __init__(self, feat_dim=256, \n",
    "                        hidden_dim_dense=128, \n",
    "                        hidden_dim_sparse=128, \n",
    "                        size=64, \n",
    "                        num_tokens=8,\n",
    "                        num_classes=1):\n",
    "                \n",
    "        super(Prototype_Prompt_Encoder, self).__init__()\n",
    "        self.dense_fc_1 = nn.Conv2d(feat_dim, hidden_dim_dense, 1)\n",
    "        self.dense_fc_2 = nn.Conv2d(hidden_dim_dense, feat_dim, 1)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.sparse_fc_1 = nn.Conv1d(size*size, hidden_dim_sparse, 1)\n",
    "        self.sparse_fc_2 = nn.Conv1d(hidden_dim_sparse, num_tokens, 1)\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        \n",
    "        pn_cls_embeddings = [nn.Embedding(num_tokens, feat_dim) for _ in range(2)] # one for positive and one for negative \n",
    "\n",
    "            \n",
    "        self.pn_cls_embeddings = nn.ModuleList(pn_cls_embeddings)\n",
    "                \n",
    "    def forward(self, feat, prototypes, cls_ids):\n",
    "  \n",
    "        cls_prompts = prototypes\n",
    "        # cls_prompts = torch.stack([cls_prompts for _ in range(feat.size(0))], dim=0)\n",
    "        \n",
    "        # feat = torch.stack([feat for _ in range(cls_prompts.size(1))], dim=1)\n",
    "\n",
    "        print(feat.shape, cls_prompts.shape, cls_ids.shape)\n",
    "        # compute similarity matrix \n",
    "        sim = torch.matmul(feat, cls_prompts)\n",
    "        feat = feat.unsqueeze(3)              # [1, 1, 4096, 1, 256]\n",
    "        sim = sim.unsqueeze(-1)               # [1, 1, 4096, 3, 1]\n",
    "\n",
    "        # compute class-activated feature\n",
    "        feat =  feat + feat*sim\n",
    "        feat_sparse = feat.clone()        \n",
    "\n",
    "        feat_selected = feat[:, 0, :, cls_ids, :] \n",
    "\n",
    "        feat = rearrange(feat_selected,'b (h w) c -> b c h w', h=64, w=64)\n",
    "        dense_embeddings = self.dense_fc_2(self.relu(self.dense_fc_1(feat)))\n",
    "        \n",
    "        # compute sparse embeddings\n",
    "        feat_sparse = feat_sparse.squeeze(1)\n",
    "        feat_sparse = rearrange(feat_sparse,'b hw num_cls c -> (b num_cls) hw c')\n",
    "        sparse_embeddings = self.sparse_fc_2(self.relu(self.sparse_fc_1(feat_sparse)))\n",
    "        sparse_embeddings = rearrange(sparse_embeddings,'(b num_cls) n c -> b num_cls n c', num_cls=self.num_classes)\n",
    "        \n",
    "        pos_embed = self.pn_cls_embeddings[1].weight.unsqueeze(0).unsqueeze(0)\n",
    "        neg_embed = self.pn_cls_embeddings[0].weight.unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "\n",
    "        sparse_embeddings = sparse_embeddings + pos_embed.detach() + neg_embed.detach()\n",
    "            \n",
    "        sparse_embeddings = rearrange(sparse_embeddings,'b num_cls n c -> b (num_cls n) c')\n",
    "        \n",
    "        return dense_embeddings, sparse_embeddings\n",
    "\n",
    "class Learnable_Prototypes(nn.Module):\n",
    "    def __init__(self, num_classes=7 , feat_dim=256):\n",
    "        super(Learnable_Prototypes, self).__init__()\n",
    "        self.class_embeddings = nn.Embedding(num_classes, feat_dim)\n",
    "        \n",
    "    def forward(self):\n",
    "        return self.class_embeddings.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "891dd5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAMTrainer(pl.LightningModule):\n",
    "    def __init__(self, sam_checkpoint='model.ckpt'):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.weights=None\n",
    "        \n",
    "        model_type = \"vit_h_no_image_encoder\"\n",
    "        # model_type = \"vit_h\"\n",
    "\n",
    "        self.sam_prompt_encoder, self.sam_decoder = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "        self.sam_prompt_encoder.cuda().eval().requires_grad_(False)\n",
    "        self.sam_decoder.cuda()\n",
    "\n",
    "        self.learnable_prototypes_model = Learnable_Prototypes(num_classes = 3, \n",
    "                                                               feat_dim = 256).cuda()\n",
    "        \n",
    "        self.prototype_prompt_encoder =  Prototype_Prompt_Encoder(feat_dim = 256, \n",
    "                                                            hidden_dim_dense = 128, \n",
    "                                                            hidden_dim_sparse = 128, \n",
    "                                                            size = 64,\n",
    "                                                            num_classes=3,\n",
    "                                                            num_tokens = 2).cuda()\n",
    "\n",
    "\n",
    "        self.seg_loss = DiceLoss()\n",
    "        self.class_loss = CrossEntropyLoss(weight=self.weights)\n",
    "        self.contrastive_loss = pml_losses.NTXentLoss(temperature=0.07)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, sam_feats, cls_ids, class_embeddings, img_size):\n",
    "        prototypes = self.learnable_prototypes_model()\n",
    "        \n",
    "        if sam_feats.shape[0] == 1:\n",
    "            prototypes = prototypes.unsqueeze(0).permute(0,2,1)        \n",
    "\n",
    "        return self.model_forward_function(\n",
    "            self.prototype_prompt_encoder,\n",
    "            self.sam_prompt_encoder,\n",
    "            self.sam_decoder,\n",
    "            sam_feats,\n",
    "            prototypes,\n",
    "            cls_ids,\n",
    "        )\n",
    "\n",
    "    # forward process of the model\n",
    "    def model_forward_function(self, prototype_prompt_encoder, \n",
    "                                sam_prompt_encoder, \n",
    "                                sam_decoder, \n",
    "                                sam_feats, \n",
    "                                prototypes, \n",
    "                                cls_ids): \n",
    "\n",
    "        sam_feats = rearrange(sam_feats, 'b h w c -> b (h w) c')\n",
    "        \n",
    "        dense_embeddings, sparse_embeddings = prototype_prompt_encoder(sam_feats, prototypes, cls_ids)\n",
    "\n",
    "        pred = []\n",
    "        pred_quality = []\n",
    "\n",
    "        sam_feats = rearrange(sam_feats,'b (h w) c -> b c h w', h=64, w=64)\n",
    "    \n",
    "        for dense_embedding, sparse_embedding, features_per_image in zip(dense_embeddings.unsqueeze(1), sparse_embeddings.unsqueeze(1), sam_feats):    \n",
    "            \n",
    "            low_res_masks_per_image, mask_quality_per_image = sam_decoder(\n",
    "                    image_embeddings=features_per_image.unsqueeze(0),\n",
    "                    image_pe=sam_prompt_encoder.get_dense_pe(), \n",
    "                    sparse_prompt_embeddings=sparse_embedding,\n",
    "                    dense_prompt_embeddings=dense_embedding, \n",
    "                    multimask_output=False,\n",
    "                )\n",
    "            \n",
    "            pred.append(low_res_masks_per_image)\n",
    "            pred_quality.append(mask_quality_per_image.detach().cpu())\n",
    "            \n",
    "        pred = torch.cat(pred,dim=0).squeeze(1)\n",
    "        pred_quality = torch.cat(pred_quality,dim=0).squeeze(1)\n",
    "        \n",
    "        return pred, pred_quality\n",
    "\n",
    "\n",
    "    def postprocess_masks(self, masks, input_size, original_size):\n",
    "        \"\"\"\n",
    "        Remove padding and upscale masks to the original image size.\n",
    "\n",
    "        Arguments:\n",
    "            masks (torch.Tensor): Batched masks from the mask_decoder,\n",
    "            in BxCxHxW format.\n",
    "            input_size (tuple(int, int)): The size of the image input to the\n",
    "            model, in (H, W) format. Used to remove padding.\n",
    "            original_size (tuple(int, int)): The original size of the image\n",
    "            before resizing for input to the model, in (H, W) format.\n",
    "\n",
    "        Returns:\n",
    "            (torch.Tensor): Batched masks in BxCxHxW format, where (H, W)\n",
    "            is given by original_size.\n",
    "        \"\"\"\n",
    "        masks = F.interpolate(\n",
    "            masks,\n",
    "            (1024, 1024),\n",
    "            mode=\"bilinear\",\n",
    "            align_corners=False,\n",
    "        )\n",
    "        masks = masks[..., : input_size[0], : input_size[1]]\n",
    "        masks = F.interpolate(masks, original_size, mode=\"bilinear\", align_corners=False)\n",
    "        return masks\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        sam_feats, _, cls_ids, masks, class_embeddings, img_size = batch\n",
    "        sam_feats, cls_ids, masks, class_embeddings = sam_feats.cuda(), cls_ids.cuda(), masks.cuda(), class_embeddings.cuda()\n",
    "\n",
    "        preds, _, cls_probs = self.forward(sam_feats, cls_ids, class_embeddings, img_size[1:])\n",
    "\n",
    "        cls_loss = self.class_loss(cls_probs, cls_ids)\n",
    "        contrastive_loss = self.contrastive_loss(\n",
    "            self.learnable_prototypes_model(),\n",
    "            torch.arange(1, self.hparams.num_classes + 1).cuda(),\n",
    "            ref_emb=class_embeddings,\n",
    "            ref_labels=cls_ids\n",
    "        )\n",
    "        seg_loss = self.seg_loss(preds, masks / 255.)\n",
    "        total_loss = seg_loss + contrastive_loss + cls_loss\n",
    "\n",
    "        self.log(\"train/seg_loss\", seg_loss)\n",
    "        self.log(\"train/class_loss\", cls_loss)\n",
    "        self.log(\"train/contrastive_loss\", contrastive_loss)\n",
    "        self.log(\"train/loss\", total_loss)\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        sam_feats, mask_names, cls_ids, masks, class_embeddings = batch\n",
    "        sam_feats, cls_ids = sam_feats.cuda(), cls_ids.cuda()\n",
    "        preds, preds_quality, cls_probs = self.forward(sam_feats, cls_ids, class_embeddings, img_size[1:])\n",
    "\n",
    "        cls_loss = self.class_loss(cls_probs, cls_ids)\n",
    "        contrastive_loss = self.contrastive_loss(\n",
    "            self.learnable_prototypes_model(),\n",
    "            torch.arange(1, self.hparams.num_classes + 1).cuda(),\n",
    "            ref_emb=class_embeddings,\n",
    "            ref_labels=cls_ids\n",
    "        )\n",
    "        seg_loss = self.seg_loss(preds, masks.cuda() / 255.)\n",
    "        total_loss = seg_loss + contrastive_loss + cls_loss\n",
    "\n",
    "        self.log(\"val/seg_loss\", seg_loss)\n",
    "        self.log(\"val/class_loss\", cls_loss)\n",
    "        self.log(\"val/contrastive_loss\", contrastive_loss)\n",
    "        self.log(\"val/loss\", total_loss)\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam([\n",
    "            {'params': self.learnable_prototypes_model.parameters()},\n",
    "            {'params': self.prototype_prompt_encoder.parameters()},\n",
    "            {'params': self.sam_decoder.parameters()}\n",
    "        ], lr=self.hparams.lr, weight_decay=0.0001)\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1212032c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "5611f672",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SAMTrainer(sam_checkpoint=sam_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "2f443b9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 64, 64, 256]),\n",
       " torch.Size([1, 3]),\n",
       " (3, 480, 836),\n",
       " torch.Size([1, 256, 3]))"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat, labels, masks, class_embedding, img_size = test_ds[0]\n",
    "feat.shape, labels.shape, masks.shape, class_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "6d17d768",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_embedding = class_embedding.permute(2,1,0)\n",
    "labels = labels.permute(1,0)\n",
    "feats = torch.cat([feat, feat, feat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb58101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4096, 256]) torch.Size([3, 256]) torch.Size([3, 1])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (12288x256 and 3x256)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[126], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_embedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tools/anaconda3/envs/flyby/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[122], line 36\u001b[0m, in \u001b[0;36mSAMTrainer.forward\u001b[0;34m(self, sam_feats, cls_ids, class_embeddings, img_size)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sam_feats\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     34\u001b[0m     prototypes \u001b[38;5;241m=\u001b[39m prototypes\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m1\u001b[39m)        \n\u001b[0;32m---> 36\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_forward_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprototype_prompt_encoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msam_prompt_encoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msam_decoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43msam_feats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprototypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcls_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[122], line 55\u001b[0m, in \u001b[0;36mSAMTrainer.model_forward_function\u001b[0;34m(self, prototype_prompt_encoder, sam_prompt_encoder, sam_decoder, sam_feats, prototypes, cls_ids)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmodel_forward_function\u001b[39m(\u001b[38;5;28mself\u001b[39m, prototype_prompt_encoder, \n\u001b[1;32m     47\u001b[0m                             sam_prompt_encoder, \n\u001b[1;32m     48\u001b[0m                             sam_decoder, \n\u001b[1;32m     49\u001b[0m                             sam_feats, \n\u001b[1;32m     50\u001b[0m                             prototypes, \n\u001b[1;32m     51\u001b[0m                             cls_ids): \n\u001b[1;32m     53\u001b[0m     sam_feats \u001b[38;5;241m=\u001b[39m rearrange(sam_feats, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb h w c -> b (h w) c\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 55\u001b[0m     dense_embeddings, sparse_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mprototype_prompt_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43msam_feats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprototypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcls_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m     pred \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     58\u001b[0m     pred_quality \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/tools/anaconda3/envs/flyby/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[121], line 49\u001b[0m, in \u001b[0;36mPrototype_Prompt_Encoder.forward\u001b[0;34m(self, feat, prototypes, cls_ids)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(feat\u001b[38;5;241m.\u001b[39mshape, cls_prompts\u001b[38;5;241m.\u001b[39mshape, cls_ids\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# compute similarity matrix \u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m sim \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcls_prompts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m feat \u001b[38;5;241m=\u001b[39m feat\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m3\u001b[39m)              \u001b[38;5;66;03m# [1, 1, 4096, 1, 256]\u001b[39;00m\n\u001b[1;32m     51\u001b[0m sim \u001b[38;5;241m=\u001b[39m sim\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)               \u001b[38;5;66;03m# [1, 1, 4096, 3, 1]\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (12288x256 and 3x256)"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "out = model(feats, labels, class_embedding, img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d1da4579",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-157.5040, device='cuda:0', grad_fn=<MinBackward1>)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9cf36c23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.3917]), (3, 480, 836))"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[1], masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5687cce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute loss \n",
    "contrastive_loss = model.contrastive_loss_model(prototypes, torch.tensor([i for i in range(1, prototypes.size()[0] + 1)]).cuda(), ref_emb = class_embeddings, ref_labels = cls_ids)\n",
    "seg_loss = model.seg_loss_model(preds, masks/255)\n",
    "\n",
    "loss = seg_loss + contrastive_loss\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flyby",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
